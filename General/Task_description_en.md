# Self documenting API Gateway for ML serving in K8S

## The initial formulation of the problem
A typical ingress implementation in k8s forwards incoming traffic to a particular application. An improved API Gateway would integrate Single Sign-On, automatic OpenAPI schema generation, request validation and response caching. The goal of the project is to develop such a Kubernetes operator to extend existing ingress implementations to API Gateway. 
The project should provide means to define API schema using Custom Resource Definitions in K8s directly and generate them from the source code following a set of conventions.

## Sketches, excerpts from correspondence and brainstorm.

1. There is persona Hadi, there is Anton. Anton is a developer of ML models. He made a model that makes very cool things for the tower. Accordingly, a lot of people, for example Mr. Hadi, want to use this model. But the bad luck is that Anton is a good developer, but he does not know how to upload the model to the server so that it turns and works there. To do this, he has to ask a student who develops, for example, a FastAPI application that uses his model and provides a web api in order to use it from outside. And now Anton is rolling out the v2 version of the model, where there are new functions or the old ones have changed. Mr. Hadi sees that something has changed and is forced to go to Anton and find out what is there and how it works, what to tell him why and why, what is at the exit.
What we do: we give Anton a CRD template for a model, for example on tensorflow (i.e. we will probably have a set of templates for different frameworks). He fills in this template literally: there is a function A, accepts B, C, outputs D, D. Then we take his model, generate an OpenAPI scheme based on Anton's CRD, run the model and forward this usage mechanism to it through the resulting scheme. Accordingly, then Hadi comes, wants to use the model. And he doesn't have to run back to Anton - he just calls some type of / methods and is given an OpenAPI schema with a description of the methods that Anton filled in. 
If we do not know the framework in which the neural network is written, then we will parse the source code and look for methods there.
The default features that GatewayAPI should do: SSO and everything - for example, if we want to give students access to neural network methods /get_grades, and for teachers - /set_grades. Then we need to be able to authenticate people and that's where the SSO features will take effect. Caching is also understandable, I think - it will be for any request in fact.

2. There is an ML model developer who has developed it and wants to make it available to external services. To do this, he needs to deploy it somewhere. But he does not know how to do this (for example) and asks for help from another developer, who, based on the functions of the model, makes some kind of web api application that runs the model and provides access to it through, for example, http requests like /do_smth. But then the developer changes some functions or adds new ones. And now he needs to change the web server again, as well as for users to wait for documentation to understand what has changed and how.
What are we doing and what will be the new User Story: the developer wrote an ML model, wants to embed it. He comes to our service, we give him CRD templates for ML frameworks known to us (for example, tensorflow) and ask him to describe what ML functions he wants to make available, what they accept and what they give. If the framework is unknown to us, then we will parse the source code and try to build the CRD ourselves. Then the developer gives this CRD to the service, based on it we create a web api with the specified methods, which also raises the model and allows you to communicate with it. And we also generate an OpenAPI schema based on this, and for example, through a query of the form /methods, it will be able to get the supported methods and their input and output parameters. That is, this will be the Self-Documenting and OpenAPI schema generation features.
Well, the main features of the Gateway API as such are of the Response Caching type - it is clear what will be done, SSO and everything related to authorization will also be used, for example, to restrict access to certain functionality to user groups.

3. The ML model developer wants to deploy and publish his service for use. To do this, he initially has his model(s) (e.g. as a Python class compiled into a binary file from his framework, for example TensorFlow, Scikit-learn, Pytorch etc.), as well as a web api service that can run this model. We believe that the service does not just provide API methods for launching models, but solves some complex task that hides both the launch of the model(s) with various input data and various intermediate processing (which includes anything from processing data using numpy to queries to the Internet in order to collect some data). Thus, the conditional /api/v1/do_smh hides a large function that includes working with the ML model. Since existing solutions for fast model serving in containers usually solve a problem in which one model is matched with one function (for example, [KServe](https://github.com/kserve/kserve ?tab=readme-ov-file) or Seldon-core+[Ambassador](https://docs.seldon.io/projects/seldon-core/en/latest/ingress/ambassador.html ) based on it), then they will not suit our ML developer, and he will still have to write a web api service himself, using all the libraries he needs + his ML framework + api framework (e.g. [FastAPI](https://fastapi.tiangolo.com/tutorial/first-steps /), Flask, or something more specialized for its task, for example [MLRun](https://docs.mlrun.org/en/latest/index.html )). Now the developer publishes an image that includes the above mentioned and deposits it into the cluster, filling in the CRD with information about available functions (but not models). Based on it, we generate and publish an OpenAPI schema, but we do not create our own web api, but only forward requests to the web api written by the developer. If the CRD does not contain information about functions, then we try to find the functions ourselves in the source code of the developer's web api.

4. In the API Gateway task!= k8s Gateway API. You can look at the k8s Gateway API, you can use it based on nginx-gateway-fabric or another OSS certified CNCF. We need more than just routing to services. OpenAPIv3 schema generation is needed on the fly from metadata, description as a CRD resource or code (using an additional library) of an ML service. ML service is a solution to the problem of serving models. Consider that all such services should have an API built according to general rules (inputs, outputs, data types, authorization, etc.). And the model developer writes only the model, but not the service. The value is that the gateway being created collects and publishes the API in a single form for hosted services, while maintaining compatibility with non-standard services. So DevOps are no longer involved in API definition and deployment during deployment.

5. ML developer solves some kind of application problem. This means that he cannot do with a pure ml framework. Different methods of the same ml service can make different stories or different story steps that need the same model.

6. You can have a serving library that wraps the inference model or code into a service and analyzes this code.

## Detailed description of the task
The Self Documenting API Gateway being developed is a platform for deploying, publishing and managing machine learning models in Kubernetes. It consists of K8S resources.

### Consideration of candidates for features
1. Request routing. The most standard feature of the basic K8S [Ingress](https://kubernetes.io/docs/concepts/services-networking/ingress /). The reverse proxy directs external traffic to the required K8S services.
Examples: There are examples of [Gateway API](https://gateway-api.sigs.k8s.io/implementations /), the most famous are HAProxy, Istio, Ambassador Ingress. Also based on them, [Seldon Core](https://docs.seldon.io/projects/seldon-core/en/latest/workflow/github-readme.html ) is able to publish ML models immediately with API routes.
2. Load balancing. In general, Kubernetes implements it using K8S[services](https://kubernetes.io/docs/concepts/services-networking/service /), but not Ingress. As a rule, it is adjusted by scales.
Examples: [Istio](https://istio.io/latest/docs/concepts/traffic-management /). Ambassador to [Seldon Core](https://docs.seldon.io/projects/seldon-core/en/latest/ingress/ambassador.html ).
3. Audit, logging. Extension of the base nginx logs. In the future, integration with log storage and rotation tools such as ELK Stack, Fluentd.
Examples: [HAProxy](https://www.haproxy.com/blog/logging-with-the-haproxy-kubernetes-ingress-controller )
4. SSO. Authorization. A non-standard Ingress function added by the Gateway API. Allows you to regulate API access using authentication tools, such as OAuth, JWT.
Examples: [HAProxy](https://www.getambassador.io/docs/edge-stack/latest/howtos/ext-filters ), [Istio](https://istio.io/latest/docs/tasks/security /).
5. Validation of requests. An unpopular function that verifies API requests in accordance with the OpenAPI scheme. It is very well combined with the auto-generation function of the circuit.
Examples: [Yandex API Gateway](https://yandex.cloud/ru/docs/api-gateway/concepts/extensions/validator?utm_referrer=https%3A%2F%2Fwww.google.com%2F)
6. Caching of responses. It is already implemented in a simple form in nginx.
Examples: [K8S Ingress](https://kubernetes .github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#server-snippet)
7. Modular deployment of models. Support for the deployment of various machine learning (ML) models written in different programming languages and using different frameworks (TensorFlow, PyTorch, Scikit-learn, etc.).
Examples: [Seldon Core](https://docs.seldon.io/projects/seldon-core/en/latest/workflow/github-readme.html )
8. Containerization. The ability to package ML models in containers using Docker. Generating a class with ML functions. It is configured based on CRD.
Examples: [Language wrapper](https://docs.seldon.io/projects/seldon-core/en/latest/wrappers/language_wrappers.html ), [KServe](https://github.com/kserve/kserve ).
9. Service deployment. Automating the deployment of models from special repositories and version control systems in Kubernetes clusters based on standard Kubernetes objects (Deployments, Services, Pods).
Examples: [Deployment](https://docs.seldon.io/projects/seldon-core/en/latest/workflow/overview.html#seldondeployment-crd).
10. Auto-documentation of models. Generating OpenAPI specifications for models by describing models when they are deployed from resource fields, or by reading the source code (class description) a containerized application.
Examples: [OpenAPI](https://docs.seldon.io/projects/seldon-core/en/latest/reference/apis/openapi.html ), [Metadata](https://docs.seldon.io/projects/seldon-core/en/latest/reference/apis/metadata.html ).